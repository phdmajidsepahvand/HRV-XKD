# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YWg6ELusCMcBrNI_kE_AJ2SSQ5662Iei
"""

!pip install wfdb

import os

records_30_url = "https://physionet.org/files/mimic3wdb/1.0/30/RECORDS"
records_30_local = "RECORDS_30.txt"

os.system(f"wget -q -O {records_30_local} {records_30_url}")

with open(records_30_local, "r") as f:
    patient_dirs = sorted(set(line.strip().split("/")[0] for line in f if line.strip()))

print(f"ðŸ“ Total patient folders found under /30/: {len(patient_dirs)}")

N = 100
selected_dirs = patient_dirs[:N]
print(f"ðŸ” Checking first {N} patient folders...")

total_records = 0
all_records_by_folder = {}

for pid in selected_dirs:
    url = f"https://physionet.org/files/mimic3wdb/1.0/30/{pid}/RECORDS"
    local_filename = f"RECORDS_{pid}.txt"

    result = os.system(f"wget -q -O {local_filename} {url}")

    if os.path.exists(local_filename):
        with open(local_filename, "r") as f:
            records = [line.strip() for line in f if line.strip()]
        all_records_by_folder[pid] = records
        total_records += len(records)
    else:
        print(f"âš ï¸ RECORDS file missing for {pid}")

print(f"\nðŸ“¦ Total number of records found across {len(selected_dirs)} folders: {total_records}")

import os
import wfdb
import zipfile

usable_records = []
download_dir = "usable_records"
zip_filename = "usable_records.zip"
os.makedirs(download_dir, exist_ok=True)

total_checked = 0
total_usable = 0

for pid, records in all_records_by_folder.items():
    for r in records:
        total_checked += 1
        hea_url = f"https://physionet.org/files/mimic3wdb/1.0/30/{pid}/{r}.hea"
        dat_url = f"https://physionet.org/files/mimic3wdb/1.0/30/{pid}/{r}.dat"

        hea_file = os.path.join(download_dir, f"{r}.hea")
        dat_file = os.path.join(download_dir, f"{r}.dat")

        os.system(f"wget -q -O {hea_file} {hea_url}")
        os.system(f"wget -q -O {dat_file} {dat_url}")

        try:
            rec = wfdb.rdrecord(os.path.join(download_dir, r))
            channels = rec.sig_name
            has_ecg = any("ECG" in ch or "II" in ch for ch in channels)
            has_abp = any("ABP" in ch for ch in channels)

            if has_ecg and has_abp:
                usable_records.append((pid, r))
                total_usable += 1
                print(f"âœ… {r} - valid")
            else:
                os.remove(hea_file)
                os.remove(dat_file)
        except:
            if os.path.exists(hea_file): os.remove(hea_file)
            if os.path.exists(dat_file): os.remove(dat_file)

print(f"\nðŸ” Checked {total_checked} records.")
print(f"âœ… {total_usable} usable records downloaded to: {download_dir}")

with zipfile.ZipFile(zip_filename, 'w') as zipf:
    for file in os.listdir(download_dir):
        zipf.write(os.path.join(download_dir, file), arcname=file)

print(f"\nðŸ“¦ All usable files zipped into: {zip_filename}")

import os
import wfdb
import numpy as np

window_size = 3000
stride = 1000

data_windows = []
labels = []
record_names = []

usable_dir = "usable_records"
files = sorted([f[:-4] for f in os.listdir(usable_dir) if f.endswith(".hea")])

for rec_name in files:
    try:
        record = wfdb.rdrecord(os.path.join(usable_dir, rec_name))
        signals = record.p_signal
        channels = record.sig_name

        ecg_idx = next(i for i, ch in enumerate(channels) if "ECG" in ch or "II" in ch)
        abp_idx = next(i for i, ch in enumerate(channels) if "ABP" in ch)

        ecg = signals[:, ecg_idx]
        abp = signals[:, abp_idx]

        for start in range(0, len(ecg) - window_size + 1, stride):
            ecg_win = ecg[start:start+window_size]
            abp_win = abp[start:start+window_size]

            sbp = np.max(abp_win)
            dbp = np.min(abp_win)

            label = 1 if sbp > 140 or dbp > 90 else 0

            data_windows.append(ecg_win)
            labels.append(label)
            record_names.append(f"{rec_name}_{start}")

        print(f"âœ… {rec_name} processed")

    except Exception as e:
        print(f"âŒ {rec_name} skipped: {e}")

X = np.array(data_windows)
y = np.array(labels)

print(f"\nðŸ§ª Total samples: {X.shape[0]}")
print(f"ðŸ”– Label distribution: 0 â†’ {(y == 0).sum()} | 1 â†’ {(y == 1).sum()}")

X = np.where(np.isnan(X), np.mean(X[~np.isnan(X)]), X)

import numpy as np

def create_cross_window_dataset(X, y, windows_per_sample=3):
    stride = 1
    total_samples = len(X) - windows_per_sample + 1

    X_multi = []
    y_multi = []

    for i in range(0, total_samples, stride):
        x_group = X[i:i+windows_per_sample]
        y_group = y[i+windows_per_sample-1]
        if np.isnan(x_group).any():
            continue
        X_multi.append(x_group)
        y_multi.append(y_group)

    X_multi = np.array(X_multi)
    y_multi = np.array(y_multi)

    return X_multi, y_multi

X_small = X[:10000]
y_small = y[:10000]
X_cross, y_cross = create_cross_window_dataset(X_small, y_small, windows_per_sample=3)
print(" Dataset shape:", X_cross.shape)
print(" Label dist:", np.bincount(y_cross))

import torch
from torch.utils.data import Dataset

class CrossWindowECGDataset(Dataset):
    def __init__(self, X, y, windows_per_sample=3):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)
        self.wps = windows_per_sample
        self.valid_len = len(self.X) - self.wps + 1

    def __len__(self):
        return self.valid_len

    def __getitem__(self, idx):
        x_group = self.X[idx:idx+self.wps]  # [wps, 3000]
        y_label = self.y[idx + self.wps - 1]

        if torch.isnan(x_group).any():
            x_group = torch.nan_to_num(x_group, nan=0.0)

        return x_group, y_label

from sklearn.model_selection import train_test_split

X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

from torch.utils.data import DataLoader
train_dataset = CrossWindowECGDataset(X_train_raw, y_train_raw, windows_per_sample=3)
test_dataset  = CrossWindowECGDataset(X_test_raw,  y_test_raw,  windows_per_sample=3)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader  = DataLoader(test_dataset,  batch_size=64)

import torch
import torch.nn as nn

class CrossWindowAttentionModel(nn.Module):
    def __init__(self, window_size=3000, windows_per_sample=3, embed_dim=128, num_heads=4):
        super().__init__()
        self.embed = nn.Sequential(
            nn.Linear(window_size, embed_dim),
            nn.ReLU(),
        )

        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)

        self.classifier = nn.Sequential(
            nn.Linear(embed_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    def forward(self, x):  # x: [B, W, 3000]
        x = self.embed(x)  # â†’ [B, W, D]
        attn_output, _ = self.attn(x, x, x)  # self-attention â†’ [B, W, D]
        x_pooled = attn_output.mean(dim=1)  # â†’ [B, D]
        out = self.classifier(x_pooled)     # â†’ [B, 1]
        return out

pos_weight = torch.tensor([len(y_train_raw) / sum(y_train_raw)], dtype=torch.float32).to(device)

train_dataset = CrossWindowECGDataset(X_train_raw, y_train_raw)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

class CNNTeacherModel(nn.Module):
    def __init__(self):
        super(CNNTeacherModel, self).__init__()
        self.encoder = nn.Sequential(
            nn.Conv1d(3, 16, kernel_size=7, padding=3),
            nn.BatchNorm1d(16),
            nn.ReLU(),
            nn.MaxPool1d(2),

            nn.Conv1d(16, 32, kernel_size=5, padding=2),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.MaxPool1d(2),

            nn.Conv1d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(1),
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Dropout(0.3),
            nn.Linear(64, 1)
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.classifier(x)
        return torch.sigmoid(x)

model = CNNTeacherModel()
train_teacher(model, train_loader, epochs=30, pos_weight=pos_weight)

from sklearn.metrics import accuracy_score, f1_score, confusion_matrix

def evaluate_teacher(model, test_loader):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.eval()
    model.to(device)

    all_preds = []
    all_labels = []

    with torch.no_grad():
        for xb, yb in test_loader:
            xb = xb.to(device)
            yb = yb.to(device)

            preds = model(xb)
            preds = (preds > 0.5).int().squeeze()

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(yb.cpu().numpy())

    acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds)
    cm = confusion_matrix(all_labels, all_preds)

    print("ðŸ“Š Evaluation for Teacher Model")
    print(f"âœ… Accuracy: {acc*100:.2f}%")
    print(f"ðŸŽ¯ F1-score: {f1:.4f}")
    print("ðŸ§© Confusion Matrix:\n", cm)
evaluate_teacher(model, test_loader)

class StudentModel(nn.Module):
    def __init__(self, input_size=3000*3):
        super(StudentModel, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_size, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 1)
        )

    def forward(self, x):
        x = x.view(x.size(0), -1)  # [B, 3, 3000] â†’ [B, 9000]
        return torch.sigmoid(self.net(x))

def train_student_kd_fixed(student, teacher, train_loader, epochs=20, temperature=3.0, alpha=0.9, lr=1e-3):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    student.to(device)
    teacher.to(device)
    teacher.eval()

    optimizer = torch.optim.Adam(student.parameters(), lr=lr)
    bce_loss = nn.BCELoss()
    kl_loss = nn.KLDivLoss(reduction='batchmean')
    eps = 1e-6

    for epoch in range(1, epochs + 1):
        student.train()
        total_loss = 0

        for xb, yb in train_loader:
            xb = xb.to(device)
            yb = yb.to(device).float().unsqueeze(1)

            s_out = student(xb)
            s_out = torch.clamp(s_out, eps, 1 - eps)

            with torch.no_grad():
                t_out = teacher(xb)
                t_out = torch.clamp(t_out, eps, 1 - eps)

            loss_hard = bce_loss(s_out, yb)

            log_soft_ratio = torch.log(s_out / t_out)
            loss_soft = kl_loss(log_soft_ratio, t_out)

            loss = alpha * loss_soft + (1 - alpha) * loss_hard

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        print(f"ðŸŽ“ KD Epoch {epoch}/{epochs} - Loss: {total_loss:.4f}")

student_model = StudentModel()
train_student_kd_fixed(student_model, model, train_loader, epochs=20)

evaluate_teacher(student_model, test_loader)

import os
import zipfile

def zip_usable_records(folder_path='usable_records', output_zip='usable_records_backup.zip'):
    with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, _, files in os.walk(folder_path):
            for file in files:
                full_path = os.path.join(root, file)
                relative_path = os.path.relpath(full_path, folder_path)
                zipf.write(full_path, arcname=relative_path)
    print(f"âœ… ZIP created: {output_zip}")

zip_usable_records()

from google.colab import files
files.download('usable_records_backup.zip')

def train_teacher(model, train_loader, epochs=30, lr=1e-3, pos_weight=None):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight) if pos_weight else nn.BCELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)

    for epoch in range(1, epochs + 1):
        model.train()
        total_loss = 0

        for xb, yb in train_loader:
            xb = xb.to(device)                        # [B, 3, 3000]
            yb = yb.to(device).unsqueeze(1)           # [B, 1]

            output = model(xb)                        # forward pass
            loss = criterion(output, yb)              # compute loss

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        scheduler.step()
        print(f"ðŸ“š Epoch {epoch}/{epochs} - Loss: {total_loss:.4f}")

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset

class BaselineECGModel(nn.Module):
    def __init__(self, input_size=3000):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(input_size, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.model(x)

class BaselineDataset(Dataset):
    def __init__(self, X, y):
        self.X = X[:, 0, :]
        self.y = y

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

X_train_tensor = torch.tensor(X_train_raw, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train_raw, dtype=torch.float32)

X_test_tensor = torch.tensor(X_test_raw, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test_raw, dtype=torch.float32)

train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

baseline_model = BaselineECGModel().to(device)
optimizer = torch.optim.Adam(baseline_model.parameters(), lr=1e-3)
criterion = nn.BCELoss()

for epoch in range(1, 6):
    baseline_model.train()
    total_loss = 0

    for x_batch, y_batch in train_loader:
        x_batch = x_batch.to(device)
        y_batch = y_batch.to(device).unsqueeze(1)

        preds = baseline_model(x_batch)
        loss = criterion(preds, y_batch)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"ðŸ“š Epoch {epoch} - Loss: {total_loss:.4f}")

from sklearn.metrics import accuracy_score, f1_score, confusion_matrix

def evaluate_model(model, dataloader, name="Model"):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for x_batch, y_batch in dataloader:
            x_batch = x_batch.to(device)
            y_batch = y_batch.cpu().numpy()

            preds = model(x_batch).cpu().numpy()
            preds = (preds > 0.5).astype(int)

            all_preds.extend(preds.flatten())
            all_labels.extend(y_batch.flatten())

    acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds)
    cm = confusion_matrix(all_labels, all_preds)

    print(f"\nðŸ“Š Evaluation for {name}")
    print(f"âœ… Accuracy: {acc*100:.2f}%")
    print(f"ðŸŽ¯ F1-score: {f1:.4f}")
    print("ðŸ§© Confusion Matrix:\n", cm)

evaluate_model(baseline_model, test_loader, name="Baseline Model")